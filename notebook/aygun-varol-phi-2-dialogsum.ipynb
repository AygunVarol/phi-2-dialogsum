{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Technical Assignment Week 5\n\n# Aygün Varol\n\n## aygun.varol@tuni.fi","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Install Required Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:18:41.710567Z","iopub.execute_input":"2025-02-19T09:18:41.710889Z","iopub.status.idle":"2025-02-19T09:18:41.715853Z","shell.execute_reply.started":"2025-02-19T09:18:41.710863Z","shell.execute_reply":"2025-02-19T09:18:41.715025Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:18:50.349717Z","iopub.execute_input":"2025-02-19T09:18:50.350037Z","iopub.status.idle":"2025-02-19T09:18:54.527984Z","shell.execute_reply.started":"2025-02-19T09:18:50.350011Z","shell.execute_reply":"2025-02-19T09:18:54.527028Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import bitsandbytes\nimport transformers\nimport peft\nimport accelerate\nimport datasets\nimport scipy\nimport einops\nimport evaluate\nimport trl\nimport rouge_score\n\nprint(\"BitsAndBytes version:\", bitsandbytes.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"PEFT version:\", peft.__version__)\nprint(\"Accelerate version:\", accelerate.__version__)\nprint(\"Datasets version:\", datasets.__version__)\nprint(\"Scipy version:\", scipy.__version__)\nprint(\"Einops version:\", einops.__version__)\nprint(\"Evaluate version:\", evaluate.__version__)\nprint(\"TRL version:\", trl.__version__)\nprint(\"Rouge Score version:\", rouge_score.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:19:59.641044Z","iopub.execute_input":"2025-02-19T09:19:59.641381Z","iopub.status.idle":"2025-02-19T09:19:59.670091Z","shell.execute_reply.started":"2025-02-19T09:19:59.641352Z","shell.execute_reply":"2025-02-19T09:19:59.669076Z"}},"outputs":[{"name":"stdout","text":"BitsAndBytes version: 0.45.2\nTransformers version: 4.49.0\nPEFT version: 0.14.0\nAccelerate version: 1.4.0\nDatasets version: 3.3.1\nScipy version: 1.15.2\nEinops version: 0.8.1\nEvaluate version: 0.4.3\nTRL version: 0.15.1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c763d708db99>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluate version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRL version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rouge Score version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'rouge_score' has no attribute '__version__'"],"ename":"AttributeError","evalue":"module 'rouge_score' has no attribute '__version__'","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:20:17.089921Z","iopub.execute_input":"2025-02-19T09:20:17.090207Z","iopub.status.idle":"2025-02-19T09:20:17.094069Z","shell.execute_reply.started":"2025-02-19T09:20:17.090185Z","shell.execute_reply":"2025-02-19T09:20:17.093366Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Step 2: Load the Dataset from hugging face()","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:20:27.675253Z","iopub.execute_input":"2025-02-19T09:20:27.675603Z","iopub.status.idle":"2025-02-19T09:21:31.090774Z","shell.execute_reply.started":"2025-02-19T09:20:27.675575Z","shell.execute_reply":"2025-02-19T09:21:31.089847Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ········\nAdd token as git credential? (Y/n)  Y\n"},{"name":"stderr","text":"Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:25:00.550110Z","iopub.execute_input":"2025-02-19T10:25:00.550458Z","iopub.status.idle":"2025-02-19T10:25:00.554925Z","shell.execute_reply.started":"2025-02-19T10:25:00.550431Z","shell.execute_reply":"2025-02-19T10:25:00.554181Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"# https://huggingface.co/datasets/neil-code/dialogsum-test\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:25:02.314586Z","iopub.execute_input":"2025-02-19T10:25:02.314979Z","iopub.status.idle":"2025-02-19T10:25:03.360625Z","shell.execute_reply.started":"2025-02-19T10:25:02.314944Z","shell.execute_reply":"2025-02-19T10:25:03.359761Z"}},"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1999\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n})"},"metadata":{}}],"execution_count":101},{"cell_type":"code","source":"dataset['train'][0]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T09:22:37.077566Z","iopub.execute_input":"2025-02-19T09:22:37.077861Z","iopub.status.idle":"2025-02-19T09:22:37.084306Z","shell.execute_reply.started":"2025-02-19T09:22:37.077838Z","shell.execute_reply":"2025-02-19T09:22:37.083616Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Step 3: Create Bitsandbytes Configuration","metadata":{}},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:25:11.142498Z","iopub.execute_input":"2025-02-19T10:25:11.142845Z","iopub.status.idle":"2025-02-19T10:25:11.148510Z","shell.execute_reply.started":"2025-02-19T10:25:11.142819Z","shell.execute_reply":"2025-02-19T10:25:11.147761Z"}},"outputs":[],"execution_count":102},{"cell_type":"markdown","source":"## Step 4: Load the Pre-Trained Model","metadata":{}},{"cell_type":"code","source":"model_name='microsoft/phi-2'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:25:16.200817Z","iopub.execute_input":"2025-02-19T10:25:16.201154Z","iopub.status.idle":"2025-02-19T10:27:21.803685Z","shell.execute_reply.started":"2025-02-19T10:25:16.201125Z","shell.execute_reply":"2025-02-19T10:27:21.802650Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"487d61dd7108452cbe278793ef638ee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   8%|8         | 419M/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7334bbdfe1db408792f9982b9fcbac57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca3bd68fd77243acbb8802cec46b5259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f688fae88b148bda54a1f92d8bcf678"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f663dbed8388495783004c4f36811493"}},"metadata":{}}],"execution_count":103},{"cell_type":"markdown","source":"## Step 5: Tokenization","metadata":{}},{"cell_type":"code","source":"# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:33.816656Z","iopub.execute_input":"2025-02-19T10:27:33.817011Z","iopub.status.idle":"2025-02-19T10:27:35.501225Z","shell.execute_reply.started":"2025-02-19T10:27:33.816982Z","shell.execute_reply":"2025-02-19T10:27:35.500553Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"873be40a454e44a998d2a35381ae1bfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf43a9cdae8b49a59308304d56b53bdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e253ae437647fcb9820beebc828177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9386816e9ec74b3bb13da9fa5c6fc771"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"454aac26825748b6a17246e6a7c0eb47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c8d816924e4b9f91bfd083e0dbf2de"}},"metadata":{}}],"execution_count":104},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:36.660103Z","iopub.execute_input":"2025-02-19T10:27:36.660449Z","iopub.status.idle":"2025-02-19T10:27:36.665034Z","shell.execute_reply.started":"2025-02-19T10:27:36.660419Z","shell.execute_reply":"2025-02-19T10:27:36.664184Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 2788 MB.\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=100, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:38.498038Z","iopub.execute_input":"2025-02-19T10:27:38.498405Z","iopub.status.idle":"2025-02-19T10:27:38.668591Z","shell.execute_reply.started":"2025-02-19T10:27:38.498372Z","shell.execute_reply":"2025-02-19T10:27:38.667889Z"}},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":"## STEP 6: Test the Model with Zero-Shot Inference","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:40.618017Z","iopub.execute_input":"2025-02-19T10:27:40.618359Z","iopub.status.idle":"2025-02-19T10:27:43.182805Z","shell.execute_reply.started":"2025-02-19T10:27:40.618325Z","shell.execute_reply":"2025-02-19T10:27:43.182034Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nPerson1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n\nCPU times: user 2.56 s, sys: 4.9 ms, total: 2.56 s\nWall time: 2.56 s\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:47.204018Z","iopub.execute_input":"2025-02-19T10:27:47.204360Z","iopub.status.idle":"2025-02-19T10:27:47.209410Z","shell.execute_reply.started":"2025-02-19T10:27:47.204326Z","shell.execute_reply":"2025-02-19T10:27:47.208578Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:49.251149Z","iopub.execute_input":"2025-02-19T10:27:49.251480Z","iopub.status.idle":"2025-02-19T10:27:49.256559Z","shell.execute_reply.started":"2025-02-19T10:27:49.251455Z","shell.execute_reply":"2025-02-19T10:27:49.255652Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:51.313914Z","iopub.execute_input":"2025-02-19T10:27:51.314211Z","iopub.status.idle":"2025-02-19T10:27:51.319172Z","shell.execute_reply.started":"2025-02-19T10:27:51.314188Z","shell.execute_reply":"2025-02-19T10:27:51.318222Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:53.428713Z","iopub.execute_input":"2025-02-19T10:27:53.429059Z","iopub.status.idle":"2025-02-19T10:27:53.433459Z","shell.execute_reply.started":"2025-02-19T10:27:53.429029Z","shell.execute_reply":"2025-02-19T10:27:53.432539Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 2882 MB.\n","output_type":"stream"}],"execution_count":112},{"cell_type":"markdown","source":"## Step 7: Pre-process the Dataset","metadata":{}},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:27:55.159897Z","iopub.execute_input":"2025-02-19T10:27:55.160275Z","iopub.status.idle":"2025-02-19T10:28:03.498050Z","shell.execute_reply.started":"2025-02-19T10:27:55.160232Z","shell.execute_reply":"2025-02-19T10:28:03.497187Z"}},"outputs":[{"name":"stdout","text":"Found max lenth: 2048\n2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c92a917f9c8844b3bc93db5dfcd5349b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9cc3579c9b741b08abb5ad2127d6af0"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd5bc6332414b07bee5bf5a1531b559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2a84cbdf544a828ee5b8430c8b5de3"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1999, 3)\nValidation: (499, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:28:08.006205Z","iopub.execute_input":"2025-02-19T10:28:08.006564Z","iopub.status.idle":"2025-02-19T10:28:08.013410Z","shell.execute_reply.started":"2025-02-19T10:28:08.006535Z","shell.execute_reply":"2025-02-19T10:28:08.012594Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 262364160\nall model parameters: 1521392640\npercentage of trainable model parameters: 17.24%\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"print(original_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:28:10.317775Z","iopub.execute_input":"2025-02-19T10:28:10.318095Z","iopub.status.idle":"2025-02-19T10:28:10.324001Z","shell.execute_reply.started":"2025-02-19T10:28:10.318066Z","shell.execute_reply":"2025-02-19T10:28:10.323157Z"}},"outputs":[{"name":"stdout","text":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (rotary_emb): PhiRotaryEmbedding()\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}],"execution_count":115},{"cell_type":"markdown","source":"## Step 8: Prepare the Model for QLoRA","metadata":{}},{"cell_type":"code","source":"for name, module in original_model.named_modules():\n    print(name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:28:13.219123Z","iopub.execute_input":"2025-02-19T10:28:13.219444Z","iopub.status.idle":"2025-02-19T10:28:13.286105Z","shell.execute_reply.started":"2025-02-19T10:28:13.219417Z","shell.execute_reply":"2025-02-19T10:28:13.285372Z"}},"outputs":[{"name":"stdout","text":"\nmodel\nmodel.embed_tokens\nmodel.layers\nmodel.layers.0\nmodel.layers.0.self_attn\nmodel.layers.0.self_attn.q_proj\nmodel.layers.0.self_attn.k_proj\nmodel.layers.0.self_attn.v_proj\nmodel.layers.0.self_attn.dense\nmodel.layers.0.mlp\nmodel.layers.0.mlp.activation_fn\nmodel.layers.0.mlp.fc1\nmodel.layers.0.mlp.fc2\nmodel.layers.0.input_layernorm\nmodel.layers.0.resid_dropout\nmodel.layers.1\nmodel.layers.1.self_attn\nmodel.layers.1.self_attn.q_proj\nmodel.layers.1.self_attn.k_proj\nmodel.layers.1.self_attn.v_proj\nmodel.layers.1.self_attn.dense\nmodel.layers.1.mlp\nmodel.layers.1.mlp.activation_fn\nmodel.layers.1.mlp.fc1\nmodel.layers.1.mlp.fc2\nmodel.layers.1.input_layernorm\nmodel.layers.1.resid_dropout\nmodel.layers.2\nmodel.layers.2.self_attn\nmodel.layers.2.self_attn.q_proj\nmodel.layers.2.self_attn.k_proj\nmodel.layers.2.self_attn.v_proj\nmodel.layers.2.self_attn.dense\nmodel.layers.2.mlp\nmodel.layers.2.mlp.activation_fn\nmodel.layers.2.mlp.fc1\nmodel.layers.2.mlp.fc2\nmodel.layers.2.input_layernorm\nmodel.layers.2.resid_dropout\nmodel.layers.3\nmodel.layers.3.self_attn\nmodel.layers.3.self_attn.q_proj\nmodel.layers.3.self_attn.k_proj\nmodel.layers.3.self_attn.v_proj\nmodel.layers.3.self_attn.dense\nmodel.layers.3.mlp\nmodel.layers.3.mlp.activation_fn\nmodel.layers.3.mlp.fc1\nmodel.layers.3.mlp.fc2\nmodel.layers.3.input_layernorm\nmodel.layers.3.resid_dropout\nmodel.layers.4\nmodel.layers.4.self_attn\nmodel.layers.4.self_attn.q_proj\nmodel.layers.4.self_attn.k_proj\nmodel.layers.4.self_attn.v_proj\nmodel.layers.4.self_attn.dense\nmodel.layers.4.mlp\nmodel.layers.4.mlp.activation_fn\nmodel.layers.4.mlp.fc1\nmodel.layers.4.mlp.fc2\nmodel.layers.4.input_layernorm\nmodel.layers.4.resid_dropout\nmodel.layers.5\nmodel.layers.5.self_attn\nmodel.layers.5.self_attn.q_proj\nmodel.layers.5.self_attn.k_proj\nmodel.layers.5.self_attn.v_proj\nmodel.layers.5.self_attn.dense\nmodel.layers.5.mlp\nmodel.layers.5.mlp.activation_fn\nmodel.layers.5.mlp.fc1\nmodel.layers.5.mlp.fc2\nmodel.layers.5.input_layernorm\nmodel.layers.5.resid_dropout\nmodel.layers.6\nmodel.layers.6.self_attn\nmodel.layers.6.self_attn.q_proj\nmodel.layers.6.self_attn.k_proj\nmodel.layers.6.self_attn.v_proj\nmodel.layers.6.self_attn.dense\nmodel.layers.6.mlp\nmodel.layers.6.mlp.activation_fn\nmodel.layers.6.mlp.fc1\nmodel.layers.6.mlp.fc2\nmodel.layers.6.input_layernorm\nmodel.layers.6.resid_dropout\nmodel.layers.7\nmodel.layers.7.self_attn\nmodel.layers.7.self_attn.q_proj\nmodel.layers.7.self_attn.k_proj\nmodel.layers.7.self_attn.v_proj\nmodel.layers.7.self_attn.dense\nmodel.layers.7.mlp\nmodel.layers.7.mlp.activation_fn\nmodel.layers.7.mlp.fc1\nmodel.layers.7.mlp.fc2\nmodel.layers.7.input_layernorm\nmodel.layers.7.resid_dropout\nmodel.layers.8\nmodel.layers.8.self_attn\nmodel.layers.8.self_attn.q_proj\nmodel.layers.8.self_attn.k_proj\nmodel.layers.8.self_attn.v_proj\nmodel.layers.8.self_attn.dense\nmodel.layers.8.mlp\nmodel.layers.8.mlp.activation_fn\nmodel.layers.8.mlp.fc1\nmodel.layers.8.mlp.fc2\nmodel.layers.8.input_layernorm\nmodel.layers.8.resid_dropout\nmodel.layers.9\nmodel.layers.9.self_attn\nmodel.layers.9.self_attn.q_proj\nmodel.layers.9.self_attn.k_proj\nmodel.layers.9.self_attn.v_proj\nmodel.layers.9.self_attn.dense\nmodel.layers.9.mlp\nmodel.layers.9.mlp.activation_fn\nmodel.layers.9.mlp.fc1\nmodel.layers.9.mlp.fc2\nmodel.layers.9.input_layernorm\nmodel.layers.9.resid_dropout\nmodel.layers.10\nmodel.layers.10.self_attn\nmodel.layers.10.self_attn.q_proj\nmodel.layers.10.self_attn.k_proj\nmodel.layers.10.self_attn.v_proj\nmodel.layers.10.self_attn.dense\nmodel.layers.10.mlp\nmodel.layers.10.mlp.activation_fn\nmodel.layers.10.mlp.fc1\nmodel.layers.10.mlp.fc2\nmodel.layers.10.input_layernorm\nmodel.layers.10.resid_dropout\nmodel.layers.11\nmodel.layers.11.self_attn\nmodel.layers.11.self_attn.q_proj\nmodel.layers.11.self_attn.k_proj\nmodel.layers.11.self_attn.v_proj\nmodel.layers.11.self_attn.dense\nmodel.layers.11.mlp\nmodel.layers.11.mlp.activation_fn\nmodel.layers.11.mlp.fc1\nmodel.layers.11.mlp.fc2\nmodel.layers.11.input_layernorm\nmodel.layers.11.resid_dropout\nmodel.layers.12\nmodel.layers.12.self_attn\nmodel.layers.12.self_attn.q_proj\nmodel.layers.12.self_attn.k_proj\nmodel.layers.12.self_attn.v_proj\nmodel.layers.12.self_attn.dense\nmodel.layers.12.mlp\nmodel.layers.12.mlp.activation_fn\nmodel.layers.12.mlp.fc1\nmodel.layers.12.mlp.fc2\nmodel.layers.12.input_layernorm\nmodel.layers.12.resid_dropout\nmodel.layers.13\nmodel.layers.13.self_attn\nmodel.layers.13.self_attn.q_proj\nmodel.layers.13.self_attn.k_proj\nmodel.layers.13.self_attn.v_proj\nmodel.layers.13.self_attn.dense\nmodel.layers.13.mlp\nmodel.layers.13.mlp.activation_fn\nmodel.layers.13.mlp.fc1\nmodel.layers.13.mlp.fc2\nmodel.layers.13.input_layernorm\nmodel.layers.13.resid_dropout\nmodel.layers.14\nmodel.layers.14.self_attn\nmodel.layers.14.self_attn.q_proj\nmodel.layers.14.self_attn.k_proj\nmodel.layers.14.self_attn.v_proj\nmodel.layers.14.self_attn.dense\nmodel.layers.14.mlp\nmodel.layers.14.mlp.activation_fn\nmodel.layers.14.mlp.fc1\nmodel.layers.14.mlp.fc2\nmodel.layers.14.input_layernorm\nmodel.layers.14.resid_dropout\nmodel.layers.15\nmodel.layers.15.self_attn\nmodel.layers.15.self_attn.q_proj\nmodel.layers.15.self_attn.k_proj\nmodel.layers.15.self_attn.v_proj\nmodel.layers.15.self_attn.dense\nmodel.layers.15.mlp\nmodel.layers.15.mlp.activation_fn\nmodel.layers.15.mlp.fc1\nmodel.layers.15.mlp.fc2\nmodel.layers.15.input_layernorm\nmodel.layers.15.resid_dropout\nmodel.layers.16\nmodel.layers.16.self_attn\nmodel.layers.16.self_attn.q_proj\nmodel.layers.16.self_attn.k_proj\nmodel.layers.16.self_attn.v_proj\nmodel.layers.16.self_attn.dense\nmodel.layers.16.mlp\nmodel.layers.16.mlp.activation_fn\nmodel.layers.16.mlp.fc1\nmodel.layers.16.mlp.fc2\nmodel.layers.16.input_layernorm\nmodel.layers.16.resid_dropout\nmodel.layers.17\nmodel.layers.17.self_attn\nmodel.layers.17.self_attn.q_proj\nmodel.layers.17.self_attn.k_proj\nmodel.layers.17.self_attn.v_proj\nmodel.layers.17.self_attn.dense\nmodel.layers.17.mlp\nmodel.layers.17.mlp.activation_fn\nmodel.layers.17.mlp.fc1\nmodel.layers.17.mlp.fc2\nmodel.layers.17.input_layernorm\nmodel.layers.17.resid_dropout\nmodel.layers.18\nmodel.layers.18.self_attn\nmodel.layers.18.self_attn.q_proj\nmodel.layers.18.self_attn.k_proj\nmodel.layers.18.self_attn.v_proj\nmodel.layers.18.self_attn.dense\nmodel.layers.18.mlp\nmodel.layers.18.mlp.activation_fn\nmodel.layers.18.mlp.fc1\nmodel.layers.18.mlp.fc2\nmodel.layers.18.input_layernorm\nmodel.layers.18.resid_dropout\nmodel.layers.19\nmodel.layers.19.self_attn\nmodel.layers.19.self_attn.q_proj\nmodel.layers.19.self_attn.k_proj\nmodel.layers.19.self_attn.v_proj\nmodel.layers.19.self_attn.dense\nmodel.layers.19.mlp\nmodel.layers.19.mlp.activation_fn\nmodel.layers.19.mlp.fc1\nmodel.layers.19.mlp.fc2\nmodel.layers.19.input_layernorm\nmodel.layers.19.resid_dropout\nmodel.layers.20\nmodel.layers.20.self_attn\nmodel.layers.20.self_attn.q_proj\nmodel.layers.20.self_attn.k_proj\nmodel.layers.20.self_attn.v_proj\nmodel.layers.20.self_attn.dense\nmodel.layers.20.mlp\nmodel.layers.20.mlp.activation_fn\nmodel.layers.20.mlp.fc1\nmodel.layers.20.mlp.fc2\nmodel.layers.20.input_layernorm\nmodel.layers.20.resid_dropout\nmodel.layers.21\nmodel.layers.21.self_attn\nmodel.layers.21.self_attn.q_proj\nmodel.layers.21.self_attn.k_proj\nmodel.layers.21.self_attn.v_proj\nmodel.layers.21.self_attn.dense\nmodel.layers.21.mlp\nmodel.layers.21.mlp.activation_fn\nmodel.layers.21.mlp.fc1\nmodel.layers.21.mlp.fc2\nmodel.layers.21.input_layernorm\nmodel.layers.21.resid_dropout\nmodel.layers.22\nmodel.layers.22.self_attn\nmodel.layers.22.self_attn.q_proj\nmodel.layers.22.self_attn.k_proj\nmodel.layers.22.self_attn.v_proj\nmodel.layers.22.self_attn.dense\nmodel.layers.22.mlp\nmodel.layers.22.mlp.activation_fn\nmodel.layers.22.mlp.fc1\nmodel.layers.22.mlp.fc2\nmodel.layers.22.input_layernorm\nmodel.layers.22.resid_dropout\nmodel.layers.23\nmodel.layers.23.self_attn\nmodel.layers.23.self_attn.q_proj\nmodel.layers.23.self_attn.k_proj\nmodel.layers.23.self_attn.v_proj\nmodel.layers.23.self_attn.dense\nmodel.layers.23.mlp\nmodel.layers.23.mlp.activation_fn\nmodel.layers.23.mlp.fc1\nmodel.layers.23.mlp.fc2\nmodel.layers.23.input_layernorm\nmodel.layers.23.resid_dropout\nmodel.layers.24\nmodel.layers.24.self_attn\nmodel.layers.24.self_attn.q_proj\nmodel.layers.24.self_attn.k_proj\nmodel.layers.24.self_attn.v_proj\nmodel.layers.24.self_attn.dense\nmodel.layers.24.mlp\nmodel.layers.24.mlp.activation_fn\nmodel.layers.24.mlp.fc1\nmodel.layers.24.mlp.fc2\nmodel.layers.24.input_layernorm\nmodel.layers.24.resid_dropout\nmodel.layers.25\nmodel.layers.25.self_attn\nmodel.layers.25.self_attn.q_proj\nmodel.layers.25.self_attn.k_proj\nmodel.layers.25.self_attn.v_proj\nmodel.layers.25.self_attn.dense\nmodel.layers.25.mlp\nmodel.layers.25.mlp.activation_fn\nmodel.layers.25.mlp.fc1\nmodel.layers.25.mlp.fc2\nmodel.layers.25.input_layernorm\nmodel.layers.25.resid_dropout\nmodel.layers.26\nmodel.layers.26.self_attn\nmodel.layers.26.self_attn.q_proj\nmodel.layers.26.self_attn.k_proj\nmodel.layers.26.self_attn.v_proj\nmodel.layers.26.self_attn.dense\nmodel.layers.26.mlp\nmodel.layers.26.mlp.activation_fn\nmodel.layers.26.mlp.fc1\nmodel.layers.26.mlp.fc2\nmodel.layers.26.input_layernorm\nmodel.layers.26.resid_dropout\nmodel.layers.27\nmodel.layers.27.self_attn\nmodel.layers.27.self_attn.q_proj\nmodel.layers.27.self_attn.k_proj\nmodel.layers.27.self_attn.v_proj\nmodel.layers.27.self_attn.dense\nmodel.layers.27.mlp\nmodel.layers.27.mlp.activation_fn\nmodel.layers.27.mlp.fc1\nmodel.layers.27.mlp.fc2\nmodel.layers.27.input_layernorm\nmodel.layers.27.resid_dropout\nmodel.layers.28\nmodel.layers.28.self_attn\nmodel.layers.28.self_attn.q_proj\nmodel.layers.28.self_attn.k_proj\nmodel.layers.28.self_attn.v_proj\nmodel.layers.28.self_attn.dense\nmodel.layers.28.mlp\nmodel.layers.28.mlp.activation_fn\nmodel.layers.28.mlp.fc1\nmodel.layers.28.mlp.fc2\nmodel.layers.28.input_layernorm\nmodel.layers.28.resid_dropout\nmodel.layers.29\nmodel.layers.29.self_attn\nmodel.layers.29.self_attn.q_proj\nmodel.layers.29.self_attn.k_proj\nmodel.layers.29.self_attn.v_proj\nmodel.layers.29.self_attn.dense\nmodel.layers.29.mlp\nmodel.layers.29.mlp.activation_fn\nmodel.layers.29.mlp.fc1\nmodel.layers.29.mlp.fc2\nmodel.layers.29.input_layernorm\nmodel.layers.29.resid_dropout\nmodel.layers.30\nmodel.layers.30.self_attn\nmodel.layers.30.self_attn.q_proj\nmodel.layers.30.self_attn.k_proj\nmodel.layers.30.self_attn.v_proj\nmodel.layers.30.self_attn.dense\nmodel.layers.30.mlp\nmodel.layers.30.mlp.activation_fn\nmodel.layers.30.mlp.fc1\nmodel.layers.30.mlp.fc2\nmodel.layers.30.input_layernorm\nmodel.layers.30.resid_dropout\nmodel.layers.31\nmodel.layers.31.self_attn\nmodel.layers.31.self_attn.q_proj\nmodel.layers.31.self_attn.k_proj\nmodel.layers.31.self_attn.v_proj\nmodel.layers.31.self_attn.dense\nmodel.layers.31.mlp\nmodel.layers.31.mlp.activation_fn\nmodel.layers.31.mlp.fc1\nmodel.layers.31.mlp.fc2\nmodel.layers.31.input_layernorm\nmodel.layers.31.resid_dropout\nmodel.rotary_emb\nmodel.embed_dropout\nmodel.final_layernorm\nlm_head\n","output_type":"stream"}],"execution_count":116},{"cell_type":"markdown","source":"## Step 9: Set Up PEFT for Fine-Tuning ","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:28:50.040855Z","iopub.execute_input":"2025-02-19T10:28:50.041160Z","iopub.status.idle":"2025-02-19T10:28:50.403075Z","shell.execute_reply.started":"2025-02-19T10:28:50.041136Z","shell.execute_reply":"2025-02-19T10:28:50.402369Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:28:52.127279Z","iopub.execute_input":"2025-02-19T10:28:52.127649Z","iopub.status.idle":"2025-02-19T10:28:52.136524Z","shell.execute_reply.started":"2025-02-19T10:28:52.127622Z","shell.execute_reply":"2025-02-19T10:28:52.135615Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 20971520\nall model parameters: 1542364160\npercentage of trainable model parameters: 1.36%\n","output_type":"stream"}],"execution_count":119},{"cell_type":"markdown","source":"## Step 10: Train PEFT Adapter","metadata":{}},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=4,  # ✅ Increased for better GPU usage\n    gradient_accumulation_steps=2,  # ✅ Reduced for speed\n    max_steps=500,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=50,  # ✅ Logs less often\n    save_strategy=\"steps\",\n    save_steps=100,  # ✅ Saves less often\n    evaluation_strategy=\"steps\",\n    eval_steps=50,  # ✅ Evaluates less often\n    do_eval=True,\n    gradient_checkpointing=False,  # ✅ Disabled for speed\n    fp16=True,  # ✅ Enables mixed precision\n    report_to=\"none\",\n    overwrite_output_dir=True,\n    group_by_length=True,\n)\n\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:29:58.688252Z","iopub.execute_input":"2025-02-19T10:29:58.688642Z","iopub.status.idle":"2025-02-19T10:29:58.726099Z","shell.execute_reply.started":"2025-02-19T10:29:58.688612Z","shell.execute_reply":"2025-02-19T10:29:58.725452Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"peft_training_args.device\npeft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:30:07.529002Z","iopub.execute_input":"2025-02-19T10:30:07.529334Z","iopub.status.idle":"2025-02-19T11:51:47.252813Z","shell.execute_reply.started":"2025-02-19T10:30:07.529274Z","shell.execute_reply":"2025-02-19T11:51:47.251872Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 1:21:13, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.422500</td>\n      <td>1.354080</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.289900</td>\n      <td>1.326990</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.332500</td>\n      <td>1.300971</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.309700</td>\n      <td>1.295359</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.323200</td>\n      <td>1.293067</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.280200</td>\n      <td>1.287866</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.299600</td>\n      <td>1.285695</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.280600</td>\n      <td>1.283262</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.267400</td>\n      <td>1.282715</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.281100</td>\n      <td>1.281463</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.3086836471557617, metrics={'train_runtime': 4898.5208, 'train_samples_per_second': 0.817, 'train_steps_per_second': 0.102, 'total_flos': 1.873980245219328e+16, 'train_loss': 1.3086836471557617, 'epoch': 2.0})"},"metadata":{}}],"execution_count":123},{"cell_type":"markdown","source":"## Step 11: Evaluate the Model Qualitatively","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()  # Enter your HF token\n\n# Upload Model & Tokenizer\npeft_model.push_to_hub(\"Aygun/phi-2-dialogsum-finetuned\")\ntokenizer.push_to_hub(\"Aygun/phi-2-dialogsum-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:53:25.555794Z","iopub.execute_input":"2025-02-19T11:53:25.556198Z","iopub.status.idle":"2025-02-19T11:53:32.367598Z","shell.execute_reply.started":"2025-02-19T11:53:25.556170Z","shell.execute_reply":"2025-02-19T11:53:32.366807Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c7bcefba6314b7b9f36c75477a6bd23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3955895b22fe4c89ab119844b1124c7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ede877aeccfa405f996a1e038c88bc11"}},"metadata":{}},{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Aygun/phi-2-dialogsum-finetuned/commit/0bc8d970f3194b492f4547c07cae4e1ac2fcd293', commit_message='Upload tokenizer', commit_description='', oid='0bc8d970f3194b492f4547c07cae4e1ac2fcd293', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Aygun/phi-2-dialogsum-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='Aygun/phi-2-dialogsum-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":125},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"microsoft/phi-2\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:53:49.126729Z","iopub.execute_input":"2025-02-19T11:53:49.127046Z","iopub.status.idle":"2025-02-19T11:53:52.934581Z","shell.execute_reply.started":"2025-02-19T11:53:49.127022Z","shell.execute_reply":"2025-02-19T11:53:52.933905Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb65c49758ca4a4789b4416ab8e27819"}},"metadata":{}}],"execution_count":126},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:53:57.093122Z","iopub.execute_input":"2025-02-19T11:53:57.093471Z","iopub.status.idle":"2025-02-19T11:53:57.259885Z","shell.execute_reply.started":"2025-02-19T11:53:57.093441Z","shell.execute_reply":"2025-02-19T11:53:57.259181Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:54:04.940577Z","iopub.execute_input":"2025-02-19T11:54:04.940872Z","iopub.status.idle":"2025-02-19T11:54:05.430029Z","shell.execute_reply.started":"2025-02-19T11:54:04.940851Z","shell.execute_reply":"2025-02-19T11:54:05.429333Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:54:10.124805Z","iopub.execute_input":"2025-02-19T11:54:10.125135Z","iopub.status.idle":"2025-02-19T11:54:14.382099Z","shell.execute_reply.started":"2025-02-19T11:54:10.125105Z","shell.execute_reply":"2025-02-19T11:54:14.381338Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\nBrian's birthday is being celebrated. #Person1# remembers Brian's birthday and invites him to have a dance. Brian thinks the party is wonderful and #Person1# thinks #Person1# looks great. They decide to have a drink together to celebrate.\n\n### End of output.\n\nCPU times: user 4.21 s, sys: 41.3 ms, total: 4.26 s\nWall time: 4.25 s\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:54:19.552556Z","iopub.execute_input":"2025-02-19T11:54:19.553064Z","iopub.status.idle":"2025-02-19T11:54:22.759947Z","shell.execute_reply.started":"2025-02-19T11:54:19.553027Z","shell.execute_reply":"2025-02-19T11:54:22.759261Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a397d0c0c19b4e588e46483a986daea3"}},"metadata":{}}],"execution_count":132},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:54:28.053333Z","iopub.execute_input":"2025-02-19T11:54:28.053627Z","iopub.status.idle":"2025-02-19T11:55:51.716820Z","shell.execute_reply.started":"2025-02-19T11:54:28.053605Z","shell.execute_reply":"2025-02-19T11:55:51.716036Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  Person 1: Ms. Dawson, I need you to take a dic...   \n1  Person 1: Ms. Dawson, I need you to take a dic...   \n2  Person 1: Ms. Dawson, I need you to take a dic...   \n3  Person1 and Person2 are discussing the traffic...   \n4  Person1 and Person2 are discussing the traffic...   \n5  Person1 and Person2 are discussing the traffic...   \n6  Person1 informs Person2 that Masha and Hero ar...   \n7  Kate informed that Masha and Hero are getting ...   \n8  Kate informed that Masha and Hero are getting ...   \n9  Person1 and Person2 are at a party, and Person...   \n\n                                peft_model_summaries  \n0  #Person1# asks Ms. Dawson to take a dictation ...  \n1  #Person1# asks Ms. Dawson to take a dictation ...  \n2  #Person1# asks Ms. Dawson to take a dictation ...  \n3  #Person2# got stuck in traffic and #Person1# s...  \n4  #Person2# got stuck in traffic and #Person1# s...  \n5  #Person2# got stuck in traffic and #Person1# s...  \n6  Masha and Hero are getting divorced. Masha and...  \n7  Masha and Hero are getting divorced. They have...  \n8  Masha and Hero are getting divorced. They have...  \n9  Brian's birthday party is going well. #Person1...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>Person1 and Person2 are discussing the traffic...</td>\n      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Person1 informs Person2 that Masha and Hero ar...</td>\n      <td>Masha and Hero are getting divorced. Masha and...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>Kate informed that Masha and Hero are getting ...</td>\n      <td>Masha and Hero are getting divorced. They have...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Kate informed that Masha and Hero are getting ...</td>\n      <td>Masha and Hero are getting divorced. They have...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>Person1 and Person2 are at a party, and Person...</td>\n      <td>Brian's birthday party is going well. #Person1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":133},{"cell_type":"markdown","source":"## Step 12: Evaluate the Model Quantitatively (ROUGE Metric)","metadata":{}},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:56:00.826542Z","iopub.execute_input":"2025-02-19T11:56:00.827048Z","iopub.status.idle":"2025-02-19T11:56:04.345118Z","shell.execute_reply.started":"2025-02-19T11:56:00.827008Z","shell.execute_reply":"2025-02-19T11:56:04.344052Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n","output_type":"stream"}],"execution_count":134},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:56:07.921820Z","iopub.execute_input":"2025-02-19T11:56:07.922131Z","iopub.status.idle":"2025-02-19T11:56:09.322611Z","shell.execute_reply.started":"2025-02-19T11:56:07.922107Z","shell.execute_reply":"2025-02-19T11:56:09.321794Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f88d3c3e0e643e9ba09db382a8e958e"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2990526195120211, 'rouge2': 0.10874019046839419, 'rougeL': 0.21186900909813286, 'rougeLsum': 0.22342464591439556}\nPEFT MODEL:\n{'rouge1': 0.3132817683433486, 'rouge2': 0.1070363134080079, 'rougeL': 0.23226760188839027, 'rougeLsum': 0.25947902747914586}\n","output_type":"stream"}],"execution_count":135},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:56:13.268231Z","iopub.execute_input":"2025-02-19T11:56:13.268676Z","iopub.status.idle":"2025-02-19T11:56:13.275075Z","shell.execute_reply.started":"2025-02-19T11:56:13.268620Z","shell.execute_reply":"2025-02-19T11:56:13.274371Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 1.42%\nrouge2: -0.17%\nrougeL: 2.04%\nrougeLsum: 3.61%\n","output_type":"stream"}],"execution_count":136},{"cell_type":"markdown","source":"## Step 13: Save and Upload the Model to Hugging Face","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()  # Enter your HF token\n\n# Upload Model & Tokenizer\npeft_model.push_to_hub(\"Aygun/phi-2-dialogsum-finetuned\")\ntokenizer.push_to_hub(\"Aygun/phi-2-dialogsum-finetuned\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 14: Capture and Document Results","metadata":{}},{"cell_type":"markdown","source":"## Step 15: Submit the Assignment","metadata":{}}]}